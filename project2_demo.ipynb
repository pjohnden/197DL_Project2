{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peter John Enriquez\n",
    "# 2019-03086\n",
    "# ECE 197 DL Z-MWZ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 2 - Zero shot Key Word Spotting (KWS) using ImageBind\n",
    "`ImageBind` is a large multimodal model which learns a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. It enables novel emergent applications ‘out-of-the-box’ including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. It has demonstrated competitive zero-shot capabilities and here, we are to test its accuracy in terms of zero-shot keyword spotting.\n",
    "\n",
    "`KWS`, using the `speech commands v2` dataset, is made of 37-category single word utterances like \"Yes\", \"No\", \"Left\", \"Right\", etc (including silence and unknown) and can be downloaded from torchaudio datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "%pip install torchvision\n",
    "%pip install torchaudio\n",
    "\n",
    "# add. for ImageBind\n",
    "%pip install pytorchvideo \n",
    "%pip install timm\n",
    "%pip install ftfy\n",
    "%pip install regex\n",
    "%pip install einops\n",
    "%pip install fvcore\n",
    "%pip install decord\n",
    "%pip install iopath\n",
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install types-regex\n",
    "%pip install mayavi\n",
    "%pip install cartopy\n",
    "\n",
    "# add. for UI\n",
    "%pip install gradio\n",
    "%sudo apt-get install libportaudio2\n",
    "%pip install sounddevice\n",
    "\n",
    "# add. for validation run\n",
    "%pip install torchmetrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pjohn\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pjohn\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# add. for ImageBind\n",
    "import data\n",
    "import torch\n",
    "from models import imagebind_model\n",
    "from models.imagebind_model import ModalityType\n",
    "\n",
    "# add. for KWS dataset\n",
    "import os\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import torchaudio.datasets\n",
    "\n",
    "# add. for inputs\n",
    "import gradio as gr\n",
    "import random\n",
    "\n",
    "# add. for validation run\n",
    "from torchmetrics import Accuracy\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directory Arrangement\n",
    "The following directory arrangement was used to run this code. If errors regarding directories are encountered, you can review the following:\n",
    "* .assets\n",
    "  * bird_audio.wav\n",
    "  ...\n",
    "* .checkpoints\n",
    "  * imagebind_huge.pth\n",
    "* bpe\n",
    "  * bpe_simple_vocab_16e6.txt.gz\n",
    "* data\n",
    "  * speech_commands\n",
    "    * SpeechCommands\n",
    "      * speech_commands_v0.02\n",
    "        * _background_noise_\n",
    "* models\n",
    "  * \\_\\_init\\_\\_.py\n",
    "* test_log\n",
    "  * validation_run1\n",
    "  * validation_run2\n",
    "* data.py  \n",
    "* project2_demo.ipynb\n",
    "* project2_valtest.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageBind Zero-Shot Key Word Spotting Demonstration\n",
    "The following code is a demonstration of ImageBind that takes in an input from the user. An audio from the KWS test split can be randomly picked as input by checking the checkbox or the user can record his/her own voice by clicking the 'record from microphone' button. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Large gap between audio n_frames(98) and target_length (204). Is the audio_target_length setting correct?\n",
      "WARNING:root:Large gap between audio n_frames(98) and target_length (204). Is the audio_target_length setting correct?\n",
      "WARNING:root:Large gap between audio n_frames(98) and target_length (204). Is the audio_target_length setting correct?\n"
     ]
    }
   ],
   "source": [
    "## FOR DEMO\n",
    "device = \"cpu\"\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# DATASET SETUP\n",
    "CLASSES = ['silence', 'unknown', 'backward', 'bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'follow',\n",
    "        'forward', 'four', 'go', 'happy', 'house', 'learn', 'left', 'marvin', 'nine', 'no',\n",
    "        'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three',\n",
    "        'tree', 'two', 'up', 'visual', 'wow', 'yes', 'zero']\n",
    "\n",
    "# make a dictionary from CLASSES to integers\n",
    "CLASS_TO_IDX = {c: i for i, c in enumerate(CLASSES)}\n",
    "#print(CLASS_TO_IDX)\n",
    "\n",
    "data_path = os.path.join('data','speech_commands')\n",
    "if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "kws_test_dataset = torchaudio.datasets.SPEECHCOMMANDS(data_path, download=True, subset='testing')\n",
    "\n",
    "# INSTANTIATE MODEL\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "def demo(kws_state=False, audio_input=None):\n",
    "    # INPUT DATA\n",
    "    if kws_state is True: #use random kws test data\n",
    "        list_length = kws_test_dataset.__len__()\n",
    "        random_index = random.randint(0, list_length-1)\n",
    "        audio_rel_path, sample_rate , target, _, _ = kws_test_dataset.get_metadata(random_index)\n",
    "        audio_path = os.path.join(data_path, 'SpeechCommands', audio_rel_path )\n",
    "\n",
    "    elif audio_input is not None:\n",
    "        target = None\n",
    "        sample_rate = 44100 #gradio.audio default sample rate\n",
    "        audio_path = audio_input\n",
    "    \n",
    "    # LOAD DATA\n",
    "    inputs = {  ModalityType.TEXT: data.load_and_transform_text(CLASSES, device),\n",
    "                ModalityType.AUDIO: data.load_and_transform_audio_data([audio_path], device, target_length=204,sample_rate=sample_rate,num_mel_bins=128), #def. target length=204\n",
    "            } \n",
    "    with torch.no_grad():\n",
    "        embeddings = model(inputs)\n",
    "\n",
    "    text_probs = torch.softmax(embeddings[ModalityType.AUDIO] @ embeddings[ModalityType.TEXT].T, dim=-1)\n",
    "\n",
    "    index = np.argmax(text_probs.cpu().numpy())\n",
    "    prediction = CLASSES[index]\n",
    "    #print(\"Label:\", prediction) \n",
    "\n",
    "    return target, prediction, audio_path\n",
    "\n",
    "# Define the input interfaces\n",
    "button_input = gr.Checkbox(label=\"Use Random KWS Test Data\")\n",
    "audio_input = gr.Audio(label=\"Audio Input\", source=\"microphone\", type=\"filepath\")\n",
    "\n",
    "# Define the output interfaces\n",
    "output_target = gr.Textbox(label=\"Target\")\n",
    "output_prediction = gr.Textbox(label=\"Prediction\")\n",
    "output_audio = gr.Audio(label=\"Audio Output\")\n",
    "\n",
    "# Define the main Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=demo,\n",
    "    inputs=[button_input, audio_input],\n",
    "    outputs=[output_target, output_prediction, output_audio],\n",
    "    title=\"Zero-Shot Keyword Spotting using ImageBind\",\n",
    "    description=\"Enter text or record audio to make predictions.\",\n",
    "    examples=[[None, \".assets/bird_audio.wav\"],[None,\".assets/dog_audio.wav\" ],],\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "iface.launch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Run\n",
    "The following code was used to run a validation/test on a `pretrained ImageBind` using the `KWS test split`. It is the same code that `project2_valtest.py` has, and the results of the run and text file compilation of the targets and predictions are under the `test_log` folder. Two runs were made, and both runs result in the same accuracy of 0.028. The summary statistics is shown below:\n",
    "* **Validation Run 1**\n",
    "  * accuracy: 0.02762380801141262\n",
    "  * correct predictions: 304\n",
    "  * number of datapoints: 11005\n",
    "  * unaccounted data(error):0\n",
    "  * Manual accuracy calculation: 0.027623807360290777\n",
    "  * Execution time: 3796.04176735878 seconds\n",
    "* **Validation Run 2**\n",
    "  * accuracy: 0.02762380801141262\n",
    "  * correct predictions: 304\n",
    "  * number of datapoints: 11005\n",
    "  * unaccounted data(error):0\n",
    "  * Manual accuracy calculation: 0.027623807360290777\n",
    "  * Execution time: 5979.099710226059 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOR VALIDATION RUN\n",
    "start_time = time.time()\n",
    "\n",
    "#device = \"cpu\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# SETUP DATASET\n",
    "class args:\n",
    "    path = os.path.join('data','speech_commands')\n",
    "\n",
    "CLASSES = ['silence', 'unknown', 'backward', 'bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'follow',\n",
    "               'forward', 'four', 'go', 'happy', 'house', 'learn', 'left', 'marvin', 'nine', 'no',\n",
    "               'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three',\n",
    "               'tree', 'two', 'up', 'visual', 'wow', 'yes', 'zero']\n",
    "    \n",
    "# make a dictionary from CLASSES to integers\n",
    "CLASS_TO_IDX = {c: i for i, c in enumerate(CLASSES)}\n",
    "print(CLASS_TO_IDX)\n",
    "\n",
    "if not os.path.exists(args.path):\n",
    "        os.makedirs(args.path, exist_ok=True)\n",
    "\n",
    "kws_test_dataset = torchaudio.datasets.SPEECHCOMMANDS(args.path, download=True, subset='testing')\n",
    "\n",
    "\n",
    "# INSTANTIATE MODEL\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# FOR PREDICTION\n",
    "list_length = kws_test_dataset.__len__()\n",
    "targets = []\n",
    "predictions = []\n",
    "correct_count = 0\n",
    "accounted_count = 0\n",
    "error_count = 0\n",
    "for index in range(list_length):\n",
    "    try:\n",
    "        audio_rel_path, sample_rate , label, _, _ = kws_test_dataset.get_metadata(index)\n",
    "        audio_path = os.path.join(args.path, 'SpeechCommands', audio_rel_path )\n",
    "        #print('audio path: ', audio_path)\n",
    "        #print('label: ', label)\n",
    "        targets.append(CLASS_TO_IDX[label])\n",
    "        # Load data\n",
    "        inputs = {ModalityType.TEXT: data.load_and_transform_text(CLASSES, device), \n",
    "                  ModalityType.AUDIO: data.load_and_transform_audio_data([audio_path], device, clip_duration=1, target_length=204,sample_rate=sample_rate,num_mel_bins=128), #default target length=204\n",
    "        } \n",
    "\n",
    "        with torch.no_grad():\n",
    "            embeddings = model(inputs)\n",
    "\n",
    "        text_probs = torch.softmax(embeddings[ModalityType.AUDIO] @ embeddings[ModalityType.TEXT].T, dim=-1)\n",
    "        index = np.argmax(text_probs.cpu().numpy())\n",
    "        predictions.append(index)\n",
    "\n",
    "        if CLASS_TO_IDX[label]==index:\n",
    "            correct_count += 1\n",
    "    except:\n",
    "        error_count += 1\n",
    "        continue\n",
    "    accounted_count += 1\n",
    "\n",
    "# Save a Copy of the Test Run Data\n",
    "test_run_data = [targets, predictions]\n",
    "with open('test_run_data.txt', 'w') as f:\n",
    "    for sublist in test_run_data:\n",
    "        line = ' '.join([str(item) for item in sublist])\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "# Compute Accuracy\n",
    "targets = torch.tensor(targets)\n",
    "predictions = torch.tensor(predictions)\n",
    "accuracy = Accuracy('multiclass',num_classes=len(CLASSES))\n",
    "accuracy(predictions, targets)\n",
    "IMB_accuracy = accuracy.compute()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(\"Run Summary:\")\n",
    "print(f'  accuracy: {IMB_accuracy}')\n",
    "print(f'  correct predictions: {correct_count}')\n",
    "print(f'  number of datapoints: {accounted_count}')\n",
    "print(f'  unaccounted data(error):{error_count}')  \n",
    "print(f'  Manual accuracy calculation: {correct_count/accounted_count}' )\n",
    "print(f\"  Execution time: {execution_time} seconds\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "The following table shows a comparison of ImageBind Speech-to-Text performance on the `speech commands v2` dataset and other SOTA models. It can be observed that ImageBind has a noticeably poorer performance in terms of accuracy and one possible reason for this is because ImageBind is trained and tested on binding text to sound events rather than speech. It will therefore perform better when the audio input is a sound event like in the case of the example .wav files in the demo. \n",
    "\n",
    "| Models | Training Type | Evaluation | Accuracy | Notes |\n",
    "|----------|----------|----------|----------|----------|\n",
    "| ImageBind | Self-Supervised | Zero-shot | 0.028   | None | \n",
    "| [M2D](https://paperswithcode.com/paper/masked-modeling-duo-learning-representations) | Self-Supervised | Not ZS | 0.985 | None |\n",
    "| [wav2vec2-conformer-rel-pos-large ](https://huggingface.co/juliensimon/wav2vec2-conformer-rel-pos-large-finetuned-speech-commands) | Self-Supervised | Not ZS | 0.972 | None |\n",
    "| [AST-P(Gong et al.)](https://huggingface.co/MIT/ast-finetuned-speech-commands-v2) | Supervised | Not ZS | 0.981 | None |\n",
    "| [hubert-base-ls960](https://huggingface.co/superb/hubert-base-superb-ks) | Supervised | Zero-Shot | 0.963 | Limited to 10+2 classes |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_course2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
